{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Q1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "-\tLogistic Regression is a statistical and machine learning method used for binary classification problems, where the outcome is categorical, such as yes/no or 0/1.\n",
        "-\tUnlike Linear Regression, which predicts continuous values, Logistic Regression predicts probabilities and maps them to class labels using a threshold, typically 0.5.\n",
        "-\tLogistic Regression uses a logistic (sigmoid) function to squash the output of the linear model into a [0,1] range.\n",
        "-\tTherefore, the main difference lies in the type of problem they solve: Linear Regression is for regression tasks, while Logistic Regression is for classification.\n",
        "#Q2. What is the mathematical equation of Logistic Regression?\n",
        "-\tThe mathematical form of Logistic Regression is: P(Y=1∣X)=11+e−(β0+β1X1+β2X2+...+βnXn)P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n)}}P(Y=1∣X)=1+e−(β0+β1X1+β2X2+...+βnXn)1.\n",
        "-\tThis equation represents the sigmoid function applied to the linear combination of input features and their weights.\n",
        "-\tHere, β0\\beta_0β0 is the intercept, β1,β2,...,βn\\beta_1, \\beta_2, ..., \\beta_nβ1,β2,...,βn are the coefficients of the model, and X1,X2,...,XnX_1, X_2, ..., X_nX1,X2,...,Xn are the input features.\n",
        "-\tThis formulation ensures that the predicted output is a probability between 0 and 1.\n",
        "#Q3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "-\tThe sigmoid function is used in Logistic Regression because it converts any real-valued number into a value between 0 and 1.\n",
        "-\tThis is useful for modeling probabilities in binary classification, where the output must represent the likelihood of belonging to a certain class.\n",
        "-\tThe sigmoid function provides a smooth gradient which makes it suitable for optimization using gradient descent.\n",
        "-\tIt also has a simple derivative, which helps in efficiently computing gradients for training the model.\n",
        "#Q4. What is the cost function of Logistic Regression?\n",
        "-\tLogistic Regression uses the log loss (also called binary cross-entropy) as its cost function.\n",
        "-\tThe cost function is defined as: −1m∑i=1m[yilog⁡(h(xi))+(1−yi)log⁡(1−h(xi))]-\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(h(x_i)) + (1 - y_i) \\log(1 - h(x_i))]−m1∑i=1m[yilog(h(xi))+(1−yi)log(1−h(xi))], where h(xi)h(x_i)h(xi) is the predicted probability.\n",
        "-\tThis function penalizes incorrect predictions more heavily, especially those that are confident but wrong.\n",
        "-\tIt is convex, which ensures that gradient descent can efficiently find the global minimum.\n",
        "#Q5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "-\tRegularization in Logistic Regression is a technique used to prevent overfitting by penalizing large coefficients in the model.\n",
        "-\tIt adds a regularization term to the cost function, which discourages the model from becoming too complex.\n",
        "-\tRegularization helps in improving the generalization ability of the model on unseen data.\n",
        "-\tIt is especially useful when dealing with high-dimensional datasets or multicollinearity among features.\n",
        "#Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "-\tLasso Regression (L1) adds the absolute value of coefficients as a penalty term to the cost function, which can lead to sparse models by driving some coefficients to zero.\n",
        "-\tRidge Regression (L2) adds the squared value of coefficients as a penalty, which shrinks coefficients but usually does not eliminate them.\n",
        "-\tElastic Net combines both L1 and L2 penalties, balancing between feature selection (L1) and coefficient shrinkage (L2).\n",
        "-\tThe choice between these depends on the specific problem—Lasso is good for feature selection, Ridge for multicollinearity, and Elastic Net when both properties are desired.\n",
        "#Q7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "-\tElastic Net is preferred when there are multiple correlated features or when we want to perform variable selection while retaining regularization.\n",
        "-\tLasso may arbitrarily select one variable from a group of correlated variables and ignore the rest, which Elastic Net can avoid.\n",
        "-\tRidge doesn't perform feature selection but only shrinks coefficients, so Elastic Net gives a balance of both effects.\n",
        "-\tIt is particularly useful when the number of predictors is greater than the number of observations or when high-dimensional data is present.\n",
        "#Q8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "-\tThe regularization parameter λ (or C in scikit-learn, where C = 1/λ) controls the strength of the regularization applied to the model.\n",
        "-\tA larger λ value increases the penalty for large coefficients, leading to simpler models that may underfit.\n",
        "-\tA smaller λ reduces the penalty, allowing the model to fit the training data more closely but possibly leading to overfitting.\n",
        "-\tProper tuning of λ is essential to achieve a balance between bias and variance in the model.\n",
        "#Q9. What are the key assumptions of Logistic Regression?\n",
        "-\tLogistic Regression assumes that the outcome is binary and that the relationship between the log-odds of the outcome and the input variables is linear.\n",
        "-\tIt assumes independence of observations, meaning that each input instance is unrelated to others.\n",
        "-\tThe model also assumes that there is minimal multicollinearity among the independent variables.\n",
        "-\tAnother assumption is that the data is sufficiently large to ensure stable and reliable estimates of model parameters.\n",
        "#Q10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "-\tDecision Trees and Random Forests are popular alternatives that work well for both binary and multiclass classification tasks.\n",
        "-\tSupport Vector Machines (SVM) are another powerful classification algorithm, especially in high-dimensional spaces.\n",
        "-\tK-Nearest Neighbors (KNN) is a non-parametric method that makes predictions based on the closest training examples.\n",
        "-\tNeural Networks, particularly for complex and large datasets, are widely used for their flexibility and performance in classification problems.\n",
        "#Q11. What are Classification Evaluation Metrics?\n",
        "-\tClassification evaluation metrics are used to measure the performance of classification models, such as Logistic Regression.\n",
        "-\tCommon metrics include accuracy, precision, recall, F1-score, and ROC-AUC score, each offering different insights into model performance.\n",
        "-\tThese metrics help in understanding how well the model is classifying the instances, especially in cases where class imbalance exists.\n",
        "-\tChoosing the right metric depends on the specific problem and whether false positives or false negatives are more critical.\n",
        "#Q12. How does class imbalance affect Logistic Regression?\n",
        "-\tClass imbalance occurs when one class significantly outnumbers the other in a binary classification problem, causing the model to be biased toward the majority class.\n",
        "-\tLogistic Regression can produce misleadingly high accuracy by predominantly predicting the majority class, ignoring the minority class.\n",
        "-\tThis imbalance can reduce the model's ability to correctly identify the minority class, which is often more important in real-world problems like fraud detection.\n",
        "-\tSolutions include resampling techniques, using class weights, or applying alternative evaluation metrics such as ROC-AUC, precision, and recall.\n",
        "#Q13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "-\tHyperparameter tuning is the process of finding the best set of hyperparameters (like C and penalty) that optimize the model’s performance.\n",
        "-\tIn Logistic Regression, common hyperparameters include the regularization strength (C), penalty type (l1, l2, or elasticnet), and the solver used for optimization.\n",
        "-\tTechniques such as GridSearchCV or RandomizedSearchCV are used to automate the search for optimal values based on cross-validation.\n",
        "-\tProper tuning helps improve model accuracy, generalization, and stability across different datasets.\n",
        "#Q14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "-\tSolvers in Logistic Regression refer to the optimization algorithms used to minimize the cost function, and common options include 'liblinear', 'lbfgs', 'saga', and 'newton-cg'.\n",
        "-\t'liblinear' is good for small datasets and supports both L1 and L2 regularization, while 'lbfgs' and 'newton-cg' are suited for multiclass problems and support only L2.\n",
        "-\t'saga' supports both L1 and L2 regularization and works well with large datasets, including sparse data.\n",
        "-\tThe choice of solver depends on the dataset size, type of regularization, and whether the problem is binary or multiclass.\n",
        "#Q15. How is Logistic Regression extended for multiclass classification?\n",
        "-\tLogistic Regression can be extended to multiclass classification using strategies like One-vs-Rest (OvR) or Softmax (multinomial) regression.\n",
        "-\tIn the OvR approach, one classifier is trained per class against all other classes, and the class with the highest confidence score is selected.\n",
        "-\tIn Softmax regression, a generalization of Logistic Regression is used to compute the probability of each class directly using the softmax function.\n",
        "-\tMost libraries, like scikit-learn, support both methods and allow users to choose using the multi_class parameter.\n",
        "#Q16. What are the advantages and disadvantages of Logistic Regression?\n",
        "-\tLogistic Regression is simple, fast, and interpretable, making it a strong baseline model for binary classification tasks.\n",
        "-\tIt performs well when the data is linearly separable and provides probabilistic outputs, which are useful for decision-making.\n",
        "-\tHowever, it may underperform when the relationship between input and output is highly non-linear or in the presence of high-dimensional data.\n",
        "-\tIt also assumes linearity in the log-odds and independence among input features, which may not always hold true.\n",
        "#Q17. What are some use cases of Logistic Regression?\n",
        "-\tLogistic Regression is widely used in credit scoring, where it helps predict whether a customer is likely to default on a loan.\n",
        "-\tIt is also commonly applied in medical diagnosis, such as predicting whether a patient has a particular disease based on input features.\n",
        "-\tIn marketing, it helps in customer churn prediction and estimating the probability of a user clicking an ad or subscribing to a service.\n",
        "-\tAdditionally, it is used in spam detection, fraud detection, and many other binary classification applications in various industries.\n",
        "#Q18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "-\tLogistic Regression is used for binary classification problems, while Softmax Regression is its extension for multiclass classification.\n",
        "-\tIn Logistic Regression, the sigmoid function maps the output to a probability between 0 and 1 for two classes.\n",
        "-\tSoftmax Regression, on the other hand, uses the softmax function to calculate probabilities for each class and chooses the class with the highest probability.\n",
        "-\tSoftmax considers all classes simultaneously, whereas Logistic Regression (especially with OvR) focuses on separating one class from the rest at a time.\n",
        "#Q19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "-\tOne-vs-Rest (OvR) is simple to implement and interpret, and works well when classes are imbalanced or when the dataset is not too large.\n",
        "-\tSoftmax (multinomial) is better when the classes are mutually exclusive and when we want a more probabilistically sound model that considers all classes at once.\n",
        "-\tOvR trains multiple binary classifiers independently, while Softmax handles all classes jointly in a single model.\n",
        "-\tThe choice depends on the specific use case, data size, and whether computational efficiency or probabilistic accuracy is more important.\n",
        "#Q20. How do we interpret coefficients in Logistic Regression?\n",
        "-\tIn Logistic Regression, the coefficients represent the change in the log-odds of the outcome for a one-unit increase in the corresponding input variable.\n",
        "-\tA positive coefficient increases the log-odds of the output being 1, while a negative coefficient decreases it.\n",
        "-\tThe exponentiation of a coefficient (e.g., exp(coef)) gives the odds ratio, which shows how the odds change with one unit increase in the predictor.\n",
        "-\tInterpreting coefficients helps understand feature importance and directionality, especially when explaining the model to non-technical stakeholders.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_dHyigTmoPqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Binary classification\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha2yvjzaC0Zw",
        "outputId": "9b9d60d6-7d13-42a1-805d-25ef0bb7f8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L1 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbG9gP40C8J4",
        "outputId": "5c6dd442-e69a-49a8-b491-13bec3519f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 Regularization Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L2 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Model Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZJxljM9DEK1",
        "outputId": "bb0ed184-7224-4721-ca77-73196b7e224d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Regularization Accuracy: 0.956140350877193\n",
            "Model Coefficients: [[ 1.82615278e+00  9.70538186e-02  6.41959627e-02 -6.13058134e-03\n",
            "  -1.31665797e-01 -3.62358111e-01 -5.29827543e-01 -2.77361673e-01\n",
            "  -2.50553584e-01 -2.42466568e-02  2.02968663e-02  9.71481901e-01\n",
            "   1.31323552e-01 -1.07238299e-01 -7.71444794e-03  1.23136167e-03\n",
            "  -3.47202367e-02 -2.92427729e-02 -3.39942202e-02  7.73286885e-03\n",
            "   1.40739583e+00 -3.01282635e-01 -2.28908025e-01 -2.13601796e-02\n",
            "  -2.23746404e-01 -1.13481058e+00 -1.54822520e+00 -5.62982453e-01\n",
            "  -6.43847065e-01 -1.21355161e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "X = StandardScaler().fit_transform(data.data)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Elastic Net Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJAACjQ7DX8H",
        "outputId": "c21662a5-0e9a-4826-ec37-30ffd25e0c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"OvR Multiclass Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxuJ5mqmDfYA",
        "outputId": "533311b7-0fcf-4e3b-cdb3-bdc4f17ecae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvR Multiclass Accuracy: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AInn-t8CDk9R",
        "outputId": "c2379777-249e-456c-a153-8c973a137654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation.  Print the average accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(model, X, y, cv=cv)\n",
        "\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym5O5XHdEKz-",
        "outputId": "c9be13d4-91a2-470f-cca8-6d220e92cf62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [0.92982456 0.93859649 0.97368421 0.94736842 0.96460177]\n",
            "Average Accuracy: 0.9508150908244062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv('/content/logistic_regression_dataset.csv')  # Replace with actual file path\n",
        "X = df.drop('Admitted', axis=1)         # Replace 'target' with your actual target column name\n",
        "y = df['Admitted']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"CSV Dataset Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r2rq1o0EYyc",
        "outputId": "5ce7671c-ddef-42bb-a82c-a51acc13b62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV Dataset Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1EuDxj_7-dG",
        "outputId": "1a409f6b-cd80-4d60-dde3-4388b717c641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 100}\n",
            "Best Accuracy: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression())\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"OvO Multiclass Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIzQ6Phz8SNy",
        "outputId": "d006ff7a-3eef-4918-a75c-24d34a6b0fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvO Multiclass Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "50vLxgdw8WON",
        "outputId": "35693fe2-9358-4c27-c50b-664f15a42bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crWJSkB28il1",
        "outputId": "e4142067-cf62-40cc-b86c-7f0cc6647e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.984375\n",
            "Recall: 0.9402985074626866\n",
            "F1 Score: 0.9618320610687023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9], flip_y=0, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y01P4nkc8osX",
        "outputId": "69a1f68d-b491-40d9-9004-4f34e7ac9a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.93       183\n",
            "           1       0.41      0.82      0.55        17\n",
            "\n",
            "    accuracy                           0.89       200\n",
            "   macro avg       0.70      0.86      0.74       200\n",
            "weighted avg       0.93      0.89      0.90       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv(\"/content/titanic.csv\")\n",
        "data = df[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "data['age'].fillna(data['age'].median(), inplace=True)\n",
        "data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "le_sex = LabelEncoder()\n",
        "le_embarked = LabelEncoder()\n",
        "data['sex'] = le_sex.fit_transform(data['sex'])\n",
        "data['embarked'] = le_embarked.fit_transform(data['embarked'])\n",
        "\n",
        "X = data.drop('survived', axis=1)\n",
        "y = data['survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-md6pkm9yEM",
        "outputId": "b5d57ea3-09e3-49bb-d92a-921e8fdab281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8100558659217877\n",
            "\n",
            "Confusion Matrix:\n",
            " [[90 15]\n",
            " [19 55]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-d76b02b4731a>:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['age'].fillna(data['age'].median(), inplace=True)\n",
            "<ipython-input-10-d76b02b4731a>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['age'].fillna(data['age'].median(), inplace=True)\n",
            "<ipython-input-10-d76b02b4731a>:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)\n",
            "<ipython-input-10-d76b02b4731a>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)\n",
            "<ipython-input-10-d76b02b4731a>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['sex'] = le_sex.fit_transform(data['sex'])\n",
            "<ipython-input-10-d76b02b4731a>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['embarked'] = le_embarked.fit_transform(data['embarked'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Without Scaling\n",
        "model1 = LogisticRegression(solver='liblinear')\n",
        "model1.fit(X_train, y_train)\n",
        "print(\"Accuracy without Scaling:\", accuracy_score(y_test, model1.predict(X_test)))\n",
        "\n",
        "# With Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model2 = LogisticRegression(solver='liblinear')\n",
        "model2.fit(X_train_scaled, y_train)\n",
        "print(\"Accuracy with Scaling:\", accuracy_score(y_test, model2.predict(X_test_scaled)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHxTBOp7-9rn",
        "outputId": "ec5fb8ea-5502-4248-f3ac-63d670a54b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.956140350877193\n",
            "Accuracy with Scaling: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_probs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmto89Kr_NEh",
        "outputId": "02e9e24e-5a16-42b9-c487-b8b8c15772c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9977071732721914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with C=0.5\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTsGg9Eu_QwZ",
        "outputId": "acf4f532-6029-4937-d242-3a4db9c4b727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q18 Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Feature importance using coefficients\n",
        "feature_importance = pd.Series(model.coef_[0], index=data.feature_names).sort_values(ascending=False)\n",
        "print(\"Important Features based on coefficients:\\n\", feature_importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCdBsc-9AxMU",
        "outputId": "bab9c221-2698-4af5-8eaa-7fb955ad4dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features based on coefficients:\n",
            " mean radius                1.644180\n",
            "texture error              1.090758\n",
            "worst radius               0.338260\n",
            "mean texture               0.197806\n",
            "compactness error          0.084005\n",
            "concavity error            0.022333\n",
            "fractal dimension error    0.015833\n",
            "mean area                 -0.000581\n",
            "worst area                -0.017601\n",
            "mean fractal dimension    -0.022737\n",
            "smoothness error          -0.023216\n",
            "symmetry error            -0.035847\n",
            "worst perimeter           -0.037200\n",
            "concave points error      -0.037441\n",
            "area error                -0.065506\n",
            "worst fractal dimension   -0.077225\n",
            "mean perimeter            -0.081505\n",
            "radius error              -0.123378\n",
            "mean smoothness           -0.159287\n",
            "mean symmetry             -0.216655\n",
            "mean compactness          -0.296604\n",
            "worst smoothness          -0.304434\n",
            "perimeter error           -0.324526\n",
            "mean concave points       -0.365682\n",
            "worst texture             -0.408592\n",
            "mean concavity            -0.598377\n",
            "worst concave points      -0.623148\n",
            "worst compactness         -0.765960\n",
            "worst symmetry            -0.791503\n",
            "worst concavity           -1.332874\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOU5d-o9A7Wa",
        "outputId": "32d45efb-de88-4169-b95a-fe9829b75e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9053470607771504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Precision-Recall Curve\n",
        "y_scores = model.decision_function(X_test)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "2yHShLE6BHqV",
        "outputId": "24b68740-28f2-49a5-97cc-a0518cc07d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    clf = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "    acc = clf.score(X_test, y_test)\n",
        "    print(f\"Solver: {solver}, Accuracy: {acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FALO18LjBPno",
        "outputId": "7ced6509-1967-4933-fdd0-3c9b4deeecac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: saga, Accuracy: 0.9649122807017544\n",
            "Solver: lbfgs, Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XcsnsmbBXkk",
        "outputId": "2dbbf4c9-2533-4b5e-8b69-1775e077a5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9068106119605033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q 23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Raw data\n",
        "raw_model = LogisticRegression(max_iter=1000)\n",
        "raw_model.fit(X_train, y_train)\n",
        "raw_acc = raw_model.score(X_test, y_test)\n",
        "\n",
        "# Standardized data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "scaled_model = LogisticRegression(max_iter=1000)\n",
        "scaled_model.fit(X_train_scaled, y_train)\n",
        "scaled_acc = scaled_model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Accuracy on raw data: {raw_acc}\")\n",
        "print(f\"Accuracy on standardized data: {scaled_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX2h9RQeB6U7",
        "outputId": "b0efc777-41c9-4962-e0b7-16340f4edeeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.956140350877193\n",
            "Accuracy on standardized data: 0.9736842105263158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Grid search to find optimal C\n",
        "param_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best C value:\", grid.best_params_['C'])\n",
        "print(\"Best cross-validation score:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOzfLLK8CJjH",
        "outputId": "c80b072f-64c8-4165-a151-90b52f6dc82b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C value: 10\n",
            "Best cross-validation score: 0.9626373626373628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q 25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Save model\n",
        "joblib.dump(model, \"logistic_model.pkl\")\n",
        "\n",
        "# Load model\n",
        "loaded_model = joblib.load(\"logistic_model.pkl\")\n",
        "\n",
        "# Predict using loaded model\n",
        "loaded_pred = loaded_model.predict(X_test)\n",
        "print(\"Prediction from loaded model (first 5):\", loaded_pred[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ada4pmrhCpOZ",
        "outputId": "1a56cadc-865f-4151-94f1-0a33aefa2c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction from loaded model (first 5): [1 0 0 1 1]\n"
          ]
        }
      ]
    }
  ]
}